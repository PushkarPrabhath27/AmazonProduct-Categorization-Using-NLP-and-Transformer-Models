{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81391fdb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Amazon Product Categorization - Kaggle Version\n",
    "# ------------------------------\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "# Note: In Kaggle, we usually don't use Drive. We use /kaggle/input.\n",
    "SAMPLE_MODE = False       # True -> use smaller sample for quick debug\n",
    "SAMPLE_SIZE = 4000\n",
    "TOP_K = 10\n",
    "RANDOM_STATE = 42\n",
    "LSTM_EPOCHS = 3\n",
    "MAX_SEQ_LENGTH = 128\n",
    "BERT_EPOCHS = 2          # set low for debug; increase for final runs\n",
    "BERT_BATCH_SIZE = 8\n",
    "FINETUNE = False         # Set True to run BERT fine-tuning (requires GPU and time)\n",
    "\n",
    "# Defines where to save models/outputs in Kaggle\n",
    "OUTPUT_DIR = \"/kaggle/working/\" \n",
    "\n",
    "# ========== INSTALLS ==========\n",
    "print(\"INSTALL STEP: checking/installing required packages...\")\n",
    "# Kaggle has most of these, but we ensure versions here.\n",
    "!pip install -q \"tensorflow>=2.16.1,<2.20\" \n",
    "!pip install -q transformers datasets evaluate sentence-transformers\n",
    "!pip install -q scikit-learn matplotlib seaborn joblib\n",
    "# PyTorch is usually pre-installed on Kaggle GPU images, but this ensures compatibility\n",
    "!pip install -q torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import html\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ========== FILE DETECTION (KAGGLE SPECIFIC) ==========\n",
    "print(\"\\n-- Detecting Input Files in /kaggle/input --\")\n",
    "\n",
    "products_path = None\n",
    "cats_path = None\n",
    "\n",
    "# Walk through the input directory to find the files regardless of the dataset name\n",
    "for root, dirs, files in os.walk(\"/kaggle/input\"):\n",
    "    for name in files:\n",
    "        full_path = os.path.join(root, name)\n",
    "        # robust matching for products file\n",
    "        if 'product' in name.lower() and name.lower().endswith('.csv'):\n",
    "            products_path = full_path\n",
    "        # robust matching for categories file\n",
    "        if 'categor' in name.lower() and name.lower().endswith('.csv'):\n",
    "            cats_path = full_path\n",
    "\n",
    "if products_path is None or cats_path is None:\n",
    "    print(\"❌ ERROR: Could not find 'amazon_products.csv' or 'amazon_categories.csv' in /kaggle/input.\")\n",
    "    print(\"Please ensure you have clicked '+ Add Data' and attached the dataset containing these CSVs.\")\n",
    "    # List what was found to help debug\n",
    "    print(\"Files found in /kaggle/input:\")\n",
    "    for root, dirs, files in os.walk(\"/kaggle/input\"):\n",
    "        for name in files:\n",
    "            print(os.path.join(root, name))\n",
    "    raise FileNotFoundError(\"Dataset not attached or filenames do not match expected patterns.\")\n",
    "else:\n",
    "    print(f\"✅ Found Products: {products_path}\")\n",
    "    print(f\"✅ Found Categories: {cats_path}\")\n",
    "\n",
    "# ========== QUICK PREVIEW ==========\n",
    "print(\"\\n-- Previewing small samples to understand schema --\")\n",
    "prod_preview = pd.read_csv(products_path, nrows=5, low_memory=False)\n",
    "cats_preview = pd.read_csv(cats_path, nrows=20, low_memory=False)\n",
    "print(\"Products columns:\", prod_preview.columns.tolist())\n",
    "display(prod_preview)\n",
    "print(\"Categories columns:\", cats_preview.columns.tolist())\n",
    "display(cats_preview)\n",
    "\n",
    "# ========== BUILD TEXT COLUMN ==========\n",
    "print(\"\\n-- Ensuring a 'text' column exists (title + description fallback) --\")\n",
    "prod_df = pd.read_csv(products_path, low_memory=False)\n",
    "# find title / description columns heuristically\n",
    "title_cols = [c for c in prod_df.columns if c.lower() in ['title', 'product_title', 'name']]\n",
    "desc_cols  = [c for c in prod_df.columns if c.lower() in ['description', 'product_description', 'desc', 'long_description']]\n",
    "\n",
    "if 'text' not in prod_df.columns:\n",
    "    if title_cols and desc_cols:\n",
    "        prod_df['text'] = (prod_df[title_cols[0]].astype(str) + ' ' + prod_df[desc_cols[0]].astype(str)).str.strip()\n",
    "        print(\"Created 'text' from\", title_cols[0], \"+\", desc_cols[0])\n",
    "    elif title_cols:\n",
    "        prod_df['text'] = prod_df[title_cols[0]].astype(str).str.strip()\n",
    "        print(\"Created 'text' from\", title_cols[0])\n",
    "    elif desc_cols:\n",
    "        prod_df['text'] = prod_df[desc_cols[0]].astype(str).str.strip()\n",
    "        print(\"Created 'text' from\", desc_cols[0])\n",
    "    else:\n",
    "        # no text; create empty and warn\n",
    "        prod_df['text'] = ''\n",
    "        print(\"WARNING: No title/description columns found. 'text' created empty -> 0 usable rows likely.\")\n",
    "\n",
    "print(\"Sample 'text' values:\")\n",
    "display(prod_df['text'].head(5))\n",
    "\n",
    "# ========== LOAD CATEGORIES & BUILD MAP ==========\n",
    "cats_df = pd.read_csv(cats_path, low_memory=False)\n",
    "# Try to normalize expected column names\n",
    "if 'id' in cats_df.columns and 'category_name' in cats_df.columns:\n",
    "    id_col = 'id'\n",
    "    name_col = 'category_name'\n",
    "elif 'category_id' in cats_df.columns and 'category_name' in cats_df.columns:\n",
    "    id_col = 'category_id'\n",
    "    name_col = 'category_name'\n",
    "else:\n",
    "    # fallback: use first two columns\n",
    "    id_col = cats_df.columns[0]\n",
    "    name_col = cats_df.columns[1] if len(cats_df.columns) > 1 else cats_df.columns[0]\n",
    "    print(\"Warning: unexpected categories columns, using\", id_col, name_col)\n",
    "\n",
    "# Build mapping: force keys to stringified ints when possible\n",
    "def key_str(x):\n",
    "    try:\n",
    "        if pd.isna(x):\n",
    "            return None\n",
    "        fx = float(x)\n",
    "        if fx.is_integer():\n",
    "            return str(int(fx))\n",
    "        return str(x)\n",
    "    except Exception:\n",
    "        return str(x)\n",
    "\n",
    "cat_map = {}\n",
    "for i, row in cats_df.iterrows():\n",
    "    k = key_str(row[id_col])\n",
    "    v = str(row[name_col])\n",
    "    if k is not None:\n",
    "        cat_map[k] = v\n",
    "print(f\"Built cat_map with {len(cat_map)} entries (sample):\", list(cat_map.items())[:8])\n",
    "\n",
    "# ========== EXTRACT & MAP CATEGORY IDs IN PRODUCTS ==========\n",
    "print(\"\\n-- Extracting numeric token from product category column and mapping to category_name --\")\n",
    "# Determine candidate column in products (common name is 'category_id' from your preview)\n",
    "prod_candidate_cols = [c for c in prod_df.columns if c.lower() in ['category_id','category','cat_id','browse_node','browse_nodes','browseNodes']]\n",
    "prod_candidate_cols = prod_candidate_cols or [c for c in prod_df.columns if 'cat' in c.lower() or 'category' in c.lower()]\n",
    "prod_candidate_cols = list(dict.fromkeys(prod_candidate_cols))  # unique order-preserving\n",
    "print(\"Candidate product category columns:\", prod_candidate_cols)\n",
    "\n",
    "def extract_catid(val):\n",
    "    if pd.isna(val): return None\n",
    "    s = str(val).strip()\n",
    "    if s == '': return None\n",
    "    # if already pure digits\n",
    "    if re.fullmatch(r'\\d+', s):\n",
    "        return s\n",
    "    # find first numeric token inside string\n",
    "    m = re.search(r'(\\d+)', s)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    return s\n",
    "\n",
    "mapped_any = False\n",
    "if prod_candidate_cols:\n",
    "    col = prod_candidate_cols[0]\n",
    "    print(\"Using product column for mapping:\", col)\n",
    "    prod_df['category_id_extracted'] = prod_df[col].apply(extract_catid)\n",
    "    prod_df['category_name'] = prod_df['category_id_extracted'].map(cat_map)\n",
    "    mapped_any = prod_df['category_name'].notna().sum() > 0\n",
    "    print(\"Mapped rows:\", prod_df['category_name'].notna().sum(), \"out of\", len(prod_df))\n",
    "else:\n",
    "    print(\"No candidate category column detected automatically. Showing product columns for manual selection.\")\n",
    "    print(prod_df.columns.tolist())\n",
    "\n",
    "# If nothing mapped, attempt fuzzy textual mapping fallback (REQUIRES small sample & slower)\n",
    "if not mapped_any:\n",
    "    print(\"No direct numeric mapping found. Attempting fuzzy textual mapping on small sample...\")\n",
    "    from difflib import get_close_matches\n",
    "    # build category_name list\n",
    "    cat_names = list(set([str(x) for x in cats_df[name_col].astype(str).tolist()]))\n",
    "    # take up to 500 distinct product text entries and try to match\n",
    "    prod_texts = prod_df['text'].astype(str).dropna().unique()[:500]\n",
    "    fuzzy_map = {}\n",
    "    for t in prod_texts:\n",
    "        cand = get_close_matches(t, cat_names, n=1, cutoff=0.85)\n",
    "        if cand:\n",
    "            fuzzy_map[t] = cand[0]\n",
    "    if fuzzy_map:\n",
    "        print(\"Fuzzy mapping found some matches (sample):\", list(fuzzy_map.items())[:10])\n",
    "        # Here we won't assign to whole df automatically; we require a human check if many matches\n",
    "    else:\n",
    "        print(\"No strong fuzzy matches found. At this point we will filter to mapped rows only (safe).\")\n",
    "\n",
    "# Diagnostics: how many usable labeled rows (text present & category_name present)\n",
    "prod_df['text_len'] = prod_df['text'].astype(str).str.len()\n",
    "usable_df = prod_df[ (prod_df['text_len'] > 0) & (prod_df['category_name'].notna()) ].copy()\n",
    "print(\"Usable labeled rows after mapping:\", len(usable_df))\n",
    "if len(usable_df) == 0:\n",
    "    raise RuntimeError(\"No usable labeled rows after mapping. Inspect candidate columns and mapping logic above.\")\n",
    "\n",
    "# Replace prod_df with usable subset for downstream pipeline\n",
    "prod_df = usable_df.reset_index(drop=True)\n",
    "\n",
    "# Optionally sample for quicker runs\n",
    "if SAMPLE_MODE:\n",
    "    prod_df = prod_df.sample(n=min(SAMPLE_SIZE, len(prod_df)), random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "    print(\"SAMPLE_MODE active -> sampled to\", len(prod_df))\n",
    "\n",
    "print(\"Top categories (counts):\")\n",
    "print(prod_df['category_name'].value_counts().head(20))\n",
    "\n",
    "# ========== PREPROCESSING & SPLIT ==========\n",
    "print(\"\\n-- Preprocessing & train/test split --\")\n",
    "def clean_text(s):\n",
    "    if pd.isna(s): return ''\n",
    "    s = str(s)\n",
    "    s = html.unescape(s)\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'<.*?>',' ',s)\n",
    "    s = re.sub(r'http\\\\S+',' ',s)\n",
    "    s = re.sub(r'[^a-z0-9\\\\s]',' ',s)\n",
    "    s = re.sub(r'\\\\s+',' ',s).strip()\n",
    "    return s\n",
    "\n",
    "prod_df['clean_text'] = prod_df['text'].apply(clean_text)\n",
    "le = None\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "prod_df['label'] = le.fit_transform(prod_df['category_name'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = prod_df['clean_text'].values\n",
    "y = prod_df['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "print(\"Train size:\", len(X_train), \"Test size:\", len(X_test))\n",
    "\n",
    "# ========== TF-IDF + CLASSICAL MODELS ==========\n",
    "print(\"\\n-- TF-IDF & classical models (LogisticRegression, MultinomialNB, RandomForest) --\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=20000 if not SAMPLE_MODE else 2000, ngram_range=(1,2), min_df=2)\n",
    "Xtr_tfidf = tfidf.fit_transform(X_train)\n",
    "Xte_tfidf = tfidf.transform(X_test)\n",
    "print(\"TF-IDF shapes:\", Xtr_tfidf.shape, Xte_tfidf.shape)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000, solver='saga', multi_class='multinomial', random_state=RANDOM_STATE),\n",
    "    'MultinomialNB': MultinomialNB(),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=200 if not SAMPLE_MODE else 50, random_state=RANDOM_STATE, n_jobs=1)\n",
    "}\n",
    "results = {}\n",
    "for name, m in models.items():\n",
    "    print(f\"\\nTraining {name} ...\")\n",
    "    m.fit(Xtr_tfidf, y_train)\n",
    "    preds = m.predict(Xte_tfidf)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    report = classification_report(y_test, preds, output_dict=True, zero_division=0)\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    try:\n",
    "        probs = m.predict_proba(Xte_tfidf)\n",
    "        y_bin = label_binarize(y_test, classes=range(len(le.classes_)))\n",
    "        roc_macro = roc_auc_score(y_bin, probs, average='macro', multi_class='ovr')\n",
    "    except Exception:\n",
    "        roc_macro = None\n",
    "    results[name] = {'model': m, 'accuracy': acc, 'report': report, 'cm': cm, 'roc_macro': roc_macro}\n",
    "    print(f\"{name} done. Accuracy: {acc:.4f} ROC_AUC_macro: {roc_macro if roc_macro is not None else 'N/A'}\")\n",
    "\n",
    "# Summary table\n",
    "import pandas as pd\n",
    "summary_rows = []\n",
    "for name, r in results.items():\n",
    "    summary_rows.append({'Model': name, 'Accuracy': round(r['accuracy'],4), 'Macro F1': round(r['report']['macro avg']['f1-score'],4), 'ROC AUC Macro': round(r['roc_macro'],4) if r['roc_macro'] is not None else 'N/A'})\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values('Accuracy', ascending=False)\n",
    "print(\"\\nClassical model comparison:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save best classical model\n",
    "best_classical_name = summary_df.iloc[0]['Model']\n",
    "import joblib\n",
    "joblib.dump({'model': results[best_classical_name]['model'], 'tfidf': tfidf, 'label_encoder': le}, os.path.join(OUTPUT_DIR, 'best_classical_model.joblib'))\n",
    "print(f\"Saved best classical model (joblib) at {OUTPUT_DIR}best_classical_model.joblib\")\n",
    "\n",
    "# Save confusion matrix image\n",
    "import matplotlib.pyplot as plt\n",
    "cm = results[best_classical_name]['cm']\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title('Confusion Matrix - ' + best_classical_name)\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(le.classes_)), le.classes_, rotation=45, ha='right')\n",
    "plt.yticks(range(len(le.classes_)), le.classes_)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i,j], ha='center', va='center', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'confusion_classical.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved confusion matrix to {OUTPUT_DIR}confusion_classical.png\")\n",
    "\n",
    "# ========== SMALL LSTM ==========\n",
    "print(\"\\n-- Training LSTM (Keras) --\")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "\n",
    "MAX_NUM_WORDS = 20000\n",
    "MAX_LEN = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "Xtr_seq = tokenizer.texts_to_sequences(X_train)\n",
    "Xte_seq = tokenizer.texts_to_sequences(X_test)\n",
    "Xtr_pad = pad_sequences(Xtr_seq, maxlen=MAX_LEN)\n",
    "Xte_pad = pad_sequences(Xte_seq, maxlen=MAX_LEN)\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "model_lstm = Sequential([\n",
    "    Embedding(MAX_NUM_WORDS, 128, input_length=MAX_LEN),\n",
    "    LSTM(128),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "model_lstm.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"LSTM compiled. Training now...\")\n",
    "model_lstm.fit(Xtr_pad, y_train, validation_split=0.1, epochs=LSTM_EPOCHS, batch_size=32)\n",
    "pred_lstm = model_lstm.predict(Xte_pad).argmax(axis=1)\n",
    "print(\"LSTM classification report:\")\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, pred_lstm, target_names=le.classes_))\n",
    "joblib.dump({'model': model_lstm, 'tokenizer': tokenizer, 'label_encoder': le}, os.path.join(OUTPUT_DIR, 'lstm_model.joblib'))\n",
    "print(f\"Saved LSTM model at {OUTPUT_DIR}lstm_model.joblib\")\n",
    "\n",
    "# ========== BERT EMBEDDINGS + LR ==========\n",
    "print(\"\\n-- Sentence-Transformer embeddings (all-MiniLM-L6-v2) + LogisticRegression --\")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "Xtr_emb = embedder.encode(X_train, show_progress_bar=True)\n",
    "Xte_emb = embedder.encode(X_test, show_progress_bar=True)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_emb = LogisticRegression(max_iter=1000)\n",
    "clf_emb.fit(Xtr_emb, y_train)\n",
    "pred_emb = clf_emb.predict(Xte_emb)\n",
    "print(\"Embedding-based classifier report:\")\n",
    "print(classification_report(y_test, pred_emb, target_names=le.classes_))\n",
    "joblib.dump({'model': clf_emb, 'embedder_name': 'all-MiniLM-L6-v2', 'label_encoder': le}, os.path.join(OUTPUT_DIR, 'bert_embeddings_clf.joblib'))\n",
    "print(f\"Saved embeddings classifier at {OUTPUT_DIR}bert_embeddings_clf.joblib\")\n",
    "\n",
    "# ========== OPTIONAL BERT FINE-TUNING ==========\n",
    "if FINETUNE:\n",
    "    print(\"\\n-- BERT fine-tuning (HuggingFace Trainer) --\")\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "    from datasets import Dataset\n",
    "    import evaluate\n",
    "    import numpy as np\n",
    "\n",
    "    model_name = 'bert-base-uncased'\n",
    "    tokenizer_hf = AutoTokenizer.from_pretrained(model_name)\n",
    "    hf_train = Dataset.from_dict({'text': list(X_train), 'label': list(y_train)})\n",
    "    hf_test  = Dataset.from_dict({'text': list(X_test),  'label': list(y_test)})\n",
    "\n",
    "    def tokenize_fn(examples):\n",
    "        return tokenizer_hf(examples['text'], truncation=True, padding='max_length', max_length=MAX_SEQ_LENGTH)\n",
    "\n",
    "    print(\"Tokenizing datasets...\")\n",
    "    hf_train = hf_train.map(tokenize_fn, batched=True)\n",
    "    hf_test  = hf_test.map(tokenize_fn, batched=True)\n",
    "\n",
    "    hf_train = hf_train.remove_columns(['text'])\n",
    "    hf_test  = hf_test.remove_columns(['text'])\n",
    "\n",
    "    hf_train.set_format(type='torch', columns=['input_ids','attention_mask','label'])\n",
    "    hf_test.set_format(type='torch', columns=['input_ids','attention_mask','label'])\n",
    "\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        return accuracy_metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "    # Update output dir to Kaggle working dir\n",
    "    training_args = TrainingArguments(output_dir=os.path.join(OUTPUT_DIR, 'bert_finetune'), num_train_epochs=BERT_EPOCHS, per_device_train_batch_size=BERT_BATCH_SIZE, per_device_eval_batch_size=BERT_BATCH_SIZE, evaluation_strategy='epoch', save_strategy='epoch', logging_steps=50, learning_rate=2e-5, load_best_model_at_end=True, metric_for_best_model='accuracy')\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=hf_train, eval_dataset=hf_test, compute_metrics=compute_metrics)\n",
    "\n",
    "    print(\"Starting training (ensure GPU is enabled).\")\n",
    "    trainer.train()\n",
    "    trainer.save_model(os.path.join(OUTPUT_DIR, 'bert_finetuned_model'))\n",
    "    print(f\"Saved fine-tuned BERT at {OUTPUT_DIR}bert_finetuned_model\")\n",
    "\n",
    "# ========== FINAL SAVES & INFERENCE ==========\n",
    "print(\"\\n-- Saving selection summary and preparing inference helper --\")\n",
    "joblib.dump({'classical_summary': summary_df.to_dict(orient='records')}, os.path.join(OUTPUT_DIR, 'model_selection_summary.joblib'))\n",
    "\n",
    "def predict_text(text, model_key='best_classical'):\n",
    "    t = clean_text(text)\n",
    "    if model_key == 'best_classical':\n",
    "        art = joblib.load(os.path.join(OUTPUT_DIR, 'best_classical_model.joblib'))\n",
    "        model = art['model']; tf = art['tfidf']; le_local = art.get('label_encoder', le)\n",
    "        x = tf.transform([t])\n",
    "        p = model.predict(x)[0]\n",
    "        return le_local.inverse_transform([p])[0]\n",
    "    if model_key == 'bert_emb':\n",
    "        art = joblib.load(os.path.join(OUTPUT_DIR, 'bert_embeddings_clf.joblib'))\n",
    "        embedder_local = SentenceTransformer(art['embedder_name'])\n",
    "        v = embedder_local.encode([t])\n",
    "        p = art['model'].predict(v)[0]\n",
    "        return art['label_encoder'].inverse_transform([p])[0]\n",
    "    if model_key == 'lstm':\n",
    "        art = joblib.load(os.path.join(OUTPUT_DIR, 'lstm_model.joblib'))\n",
    "        tok = art['tokenizer']; mdl = art['model']; le_local = art['label_encoder']\n",
    "        seq = tok.texts_to_sequences([t]); pad = pad_sequences(seq, maxlen=MAX_LEN)\n",
    "        p = mdl.predict(pad).argmax(axis=1)[0]\n",
    "        return le_local.inverse_transform([p])[0]\n",
    "    return None\n",
    "\n",
    "print(f\"\\nDONE. Artifacts saved in {OUTPUT_DIR}:\")\n",
    "print(\" - best_classical_model.joblib\")\n",
    "print(\" - lstm_model.joblib\")\n",
    "print(\" - bert_embeddings_clf.joblib\")\n",
    "print(\" - confusion_classical.png\")\n",
    "print(\" - model_selection_summary.joblib\")\n",
    "if FINETUNE:\n",
    "    print(\" - bert_finetuned_model/\")\n",
    "\n",
    "print(\"\\nExample prediction (first product title):\")\n",
    "print(\"Text:\", prod_df['text'].iloc[0])\n",
    "print(\"Predicted (best classical):\", predict_text(prod_df['text'].iloc[0], model_key='best_classical'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
