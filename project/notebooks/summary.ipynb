{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Amazon Product Categorization - Project Summary\n",
                "\n",
                "This notebook provides a quick overview of the final project results.\n",
                "\n",
                "**Quick Summary**:\n",
                "- ✅ Achieved **96.92% test accuracy** (target: ≥85%)\n",
                "- ✅ Best Model: Logistic Regression with TF-IDF\n",
                "- ✅ Top-3 Accuracy: 99.45%\n",
                "- ✅ Complete production-ready pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Setup\n",
                "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
                "RESULTS_DIR = os.path.join(PROJECT_ROOT, \"results\")\n",
                "\n",
                "print(f\"Project root: {PROJECT_ROOT}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Test Set Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load test metrics\n",
                "metrics_df = pd.read_csv(os.path.join(RESULTS_DIR, \"metrics_test.csv\"))\n",
                "\n",
                "print(\"=\"*70)\n",
                "print(\"FINAL TEST SET RESULTS\")\n",
                "print(\"=\"*70)\n",
                "print(metrics_df.to_string(index=False))\n",
                "print(\"=\"*70)\n",
                "\n",
                "# Highlight achievement\n",
                "best_acc = metrics_df['accuracy'].max()\n",
                "if best_acc >= 0.85:\n",
                "    print(f\"\\n✅ TARGET ACHIEVED: {best_acc:.2%} >> 85% requirement\")\n",
                "else:\n",
                "    print(f\"\\n⚠️  Below target: {best_acc:.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Model Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load baseline metrics for comparison\n",
                "baseline_metrics = pd.read_csv(os.path.join(RESULTS_DIR, \"metrics_baselines.csv\"))\n",
                "\n",
                "print(\"\\nBaseline Models (Validation Set):\")\n",
                "print(baseline_metrics[['model', 'accuracy', 'macro_f1']].to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display confusion matrix\n",
                "from IPython.display import Image, display\n",
                "\n",
                "print(\"\\nConfusion Matrix (Best Baseline):\")\n",
                "display(Image(filename=os.path.join(RESULTS_DIR, \"confusion_matrix_baseline.png\")))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display ROC curves\n",
                "print(\"\\nROC Curves (Top 10 Categories):\")\n",
                "display(Image(filename=os.path.join(RESULTS_DIR, \"ROC_baseline.png\")))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Sample Predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load inference module\n",
                "sys.path.append(os.path.join(PROJECT_ROOT, \"src\"))\n",
                "from inference import predict\n",
                "\n",
                "# Test predictions\n",
                "test_products = [\n",
                "    {\"title\": \"Apple iPhone 13 Pro\", \"desc\": \"128GB, Sierra Blue, 5G\"},\n",
                "    {\"title\": \"Sony WH-1000XM4 Headphones\", \"desc\": \"Wireless, Noise Cancelling\"},\n",
                "    {\"title\": \"The Great Gatsby\", \"desc\": \"Classic novel by F. Scott Fitzgerald\"},\n",
                "]\n",
                "\n",
                "print(\"\\nSample Predictions:\\n\")\n",
                "for i, product in enumerate(test_products, 1):\n",
                "    result = predict(product['title'], product['desc'], model_type='baseline', top_k=2)\n",
                "    print(f\"{i}. Title: {product['title']}\")\n",
                "    print(f\"   Predicted: {result['predicted_category']} ({result['confidence']:.1%})\")\n",
                "    print(f\"   Top-2: {', '.join([p['category'] for p in result['top_k_predictions']])}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Key Takeaways\n",
                "\n",
                "### Achievements\n",
                "1. **Target Exceeded**: 96.92% >> 85% requirement\n",
                "2. **Robust Performance**: 96.47% Macro-F1 across all 15 categories\n",
                "3. **Excellent Top-K**: 99.45% top-3 accuracy (great for e-commerce UX)\n",
                "4. **Production Ready**: Inference pipeline, saved models, documentation\n",
                "\n",
                "### Best Practices Demonstrated\n",
                "- Stratified train/val/test splitting\n",
                "- Hyperparameter tuning via GridSearchCV\n",
                "- Multiple model comparison\n",
                "- Comprehensive evaluation metrics\n",
                "- Clean, modular code architecture\n",
                "\n",
                "### Recommendations\n",
                "1. **Deployment**: Use Logistic Regression baseline for production\n",
                "2. **Enhancement**: Complete BERT training (3-5 epochs) for potential improvements\n",
                "3. **Monitoring**: Track prediction confidence distributions in production\n",
                "4. **Extensions**: Consider ensemble methods, multi-label classification\n",
                "\n",
                "---\n",
                "\n",
                "**For complete details, see**: `REPORT/final_report.md`"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}